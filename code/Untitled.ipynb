{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import *\n",
    "import time\n",
    "\n",
    "from dataset import IntentDataset, batch_function\n",
    "from model import CapsuleNetwork\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import tool\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "a = Random()\n",
    "a.seed(1)\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "def setting(train_set, test_set, embedding):\n",
    "    vocab_size, word_emb_size = embedding.shape\n",
    "    max_time = sorted(train_set, reverse=True, key=lambda x: x['length'])[0]\n",
    "    train_num = len(train_set)\n",
    "    test_num = len(test_set)\n",
    "    s_cnum = len(train_set.class_list)\n",
    "    u_cnum = len(test_set.class_list)\n",
    "    config = {}\n",
    "    config['keep_prob'] = 0.8 # embedding dropout keep rate\n",
    "    config['hidden_size'] = 32 # embedding vector size\n",
    "    config['batch_size'] = 64 # vocab size of word vectors\n",
    "    config['vocab_size'] = vocab_size - 1 # vocab size (10895) after subtracting padding\n",
    "    config['num_epochs'] = 200 # number of epochs\n",
    "    config['max_time'] = max_time\n",
    "    config['sample_num'] = train_num # sample number of training data\n",
    "    config['test_num'] = test_num # number of test data\n",
    "    config['s_cnum'] = s_cnum # seen class num\n",
    "    config['u_cnum'] = u_cnum # unseen class num\n",
    "    config['word_emb_size'] = word_emb_size # embedding size of word vectors (300)\n",
    "    config['d_a'] = 20 # self-attention weight hidden units number\n",
    "    config['output_atoms'] = 10 #capsule output atoms\n",
    "    config['r'] = 3 #self-attention weight hops\n",
    "    config['num_routing'] = 2 #capsule routing num\n",
    "    config['alpha'] = 0.0001 # coefficient of self-attention loss\n",
    "    config['margin'] = 1.0 # ranking loss margin\n",
    "    config['learning_rate'] = 0.0001\n",
    "    config['sim_scale'] = 4 #sim scale\n",
    "    config['nlayers'] = 2 # default for bilstm\n",
    "    config['ckpt_dir'] = './saved_models/' #check point dir\n",
    "    return config\n",
    "\n",
    "def get_sim(data):\n",
    "    # get unseen and seen categories similarity\n",
    "    s = normalize(data['sc_vec'])\n",
    "    u = normalize(data['uc_vec'])\n",
    "    sim = tool.compute_label_sim(u, s, config['sim_scale'])\n",
    "    return sim\n",
    "\n",
    "def evaluate_test(data, config, lstm,embedding):\n",
    "    # zero-shot testing state\n",
    "    # seen votes shape (110, 2, 34, 10)\n",
    "    x_te = data['x_te']\n",
    "    y_te_id = data['y_te']\n",
    "    u_len = data['u_len']\n",
    "    y_ind = data['s_label']\n",
    "    # get unseen and seen categories similarity\n",
    "    # sim shape (8, 34)\n",
    "    sim_ori = torch.from_numpy(get_sim(data))\n",
    "    total_unseen_pred = np.array([], dtype=np.int64)\n",
    "    total_y_test = np.array([], dtype=np.int64)\n",
    "    batch_size  = config['test_num']\n",
    "    test_batch = int(math.ceil(config['test_num'] / float(batch_size)))\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_batch):\n",
    "            begin_index = i * batch_size\n",
    "            end_index = min((i + 1) * batch_size, config['test_num'])\n",
    "            batch_te_original = x_te[begin_index : end_index]\n",
    "            batch_len = u_len[begin_index : end_index]\n",
    "            batch_test = y_te_id[begin_index: end_index]\n",
    "            batch_len = torch.from_numpy(batch_len)\n",
    "\n",
    "            # sort by descending order for pack_padded_sequence\n",
    "            batch_len, perm_idx = batch_len.sort(0, descending=True)\n",
    "            batch_te = batch_te_original[perm_idx]\n",
    "            batch_test = batch_test[perm_idx]\n",
    "            batch_te = torch.from_numpy(batch_te)\n",
    "\n",
    "            lstm(batch_te, batch_len, embedding)\n",
    "            attentions, seen_logits, seen_votes, seen_weights_c = lstm.attention, lstm.logits, \\\n",
    "                                                                  lstm.votes, lstm.weights_c\n",
    "            sim = np.expand_dims(sim_ori,0)\n",
    "            sim =  np.tile(sim, [seen_votes.shape[1],1,1])\n",
    "            sim = np.expand_dims(sim, 0)\n",
    "            sim = np.tile(sim, [seen_votes.shape[0],1,1,1])\n",
    "            seen_weights_c = np.tile(np.expand_dims(seen_weights_c, -1), [1,1,1, config['output_atoms']])\n",
    "            mul = np.multiply(seen_votes, seen_weights_c)\n",
    "\n",
    "            # compute unseen features\n",
    "            # unseen votes shape (110, 2, 8, 10)\n",
    "            unseen_votes = np.matmul(sim, mul)\n",
    "\n",
    "            # routing unseen classes\n",
    "            u_activations, u_weights_c = update_unseen_routing(unseen_votes, config, 3)\n",
    "            unseen_logits = torch.norm(u_activations, dim=-1)\n",
    "            te_logits = unseen_logits\n",
    "            te_batch_pred = np.argmax(te_logits, 1)\n",
    "            total_unseen_pred = np.concatenate((total_unseen_pred, te_batch_pred))\n",
    "            total_y_test = np.concatenate((total_y_test, batch_test))\n",
    "            print (\"           zero-shot intent detection test set performance        \")\n",
    "            acc = accuracy_score(total_y_test, total_unseen_pred)\n",
    "            print (classification_report(total_y_test, total_unseen_pred, digits=4))\n",
    "    return acc\n",
    "\n",
    "def generate_batch(n, batch_size):\n",
    "    batch_index = a.sample(range(n), batch_size)\n",
    "    return batch_index\n",
    "\n",
    "def _squash(input_tensor):\n",
    "    norm = torch.norm(input_tensor, dim=2, keepdim=True)\n",
    "    norm_squared = norm * norm\n",
    "    return (input_tensor / norm) * (norm_squared / (0.5 + norm_squared))\n",
    "\n",
    "def update_unseen_routing(votes, config, num_routing=3):\n",
    "    votes_t_shape = [3, 0, 1, 2]\n",
    "    r_t_shape = [1, 2, 3, 0]\n",
    "    votes_trans = votes.permute(votes_t_shape)\n",
    "    num_dims = 4\n",
    "    input_dim = config['r']\n",
    "    output_dim = config['u_cnum']\n",
    "    input_shape = votes.shape\n",
    "    logit_shape = np.stack([input_shape[0], input_dim, output_dim])\n",
    "    logits = torch.zeros(logit_shape[0], logit_shape[1], logit_shape[2])\n",
    "    activations = []\n",
    "\n",
    "\n",
    "    for iteration in range(num_routing):\n",
    "        route = F.softmax(logits, dim=2)\n",
    "        preactivate_unrolled = route * votes_trans\n",
    "        preact_trans = preactivate_unrolled.permute(r_t_shape)\n",
    "\n",
    "        # delete bias to fit for unseen classes\n",
    "        preactivate = torch.sum(preact_trans, dim=1)\n",
    "        activation = _squash(preactivate)\n",
    "        # activations = activations.write(i, activation)\n",
    "        activations.append(activation)\n",
    "        # distances: [batch, input_dim, output_dim]\n",
    "        act_3d = torch.unsqueeze(activation, 1)\n",
    "        tile_shape = np.ones(num_dims, dtype=np.int32).tolist()\n",
    "        tile_shape[1] = input_dim\n",
    "        act_replicated = act_3d.repeat(tile_shape)\n",
    "        distances = torch.sum(votes * act_replicated, dim=3)\n",
    "        logits = logits + distances\n",
    "\n",
    "    return activations[num_routing-1], route\n",
    "\n",
    "def sort_batch(batch_x, batch_y, batch_len, batch_ind):\n",
    "    batch_len_new = torch.from_numpy(batch_len)\n",
    "    batch_len_new, perm_idx = batch_len_new.sort(0, descending=True)\n",
    "    batch_x_new = batch_x[perm_idx]\n",
    "    batch_y_new = batch_y[perm_idx]\n",
    "    batch_ind_new = batch_ind[perm_idx]\n",
    "\n",
    "    return torch.from_numpy(batch_x_new), torch.from_numpy(batch_y_new), \\\n",
    "           batch_len_new, torch.from_numpy(batch_ind_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 9000).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ed84009c531d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0moverall_test_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCapsuleNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dl/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         raise RuntimeError(\n\u001b[1;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dl/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mAlternatively\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m of the CUDA driver.\"\"\".format(str(torch._C._cuda_getDriverVersion())))\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 9000).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "data_prefix = '../data/nlu_data/'\n",
    "w2v_path = data_prefix + 'wiki.en.vec'\n",
    "training_data_path = data_prefix + 'train_shuffle.txt'\n",
    "test_data_path = data_prefix + 'test.txt'\n",
    "\n",
    "seen_classes = ['music', 'search', 'movie', 'weather', 'restaurant']\n",
    "unseen_classes = ['playlist', 'book']\n",
    "\n",
    "train_set = IntentDataset(seen_classes, w2v_path, training_data_path)\n",
    "test_set = IntentDataset(unseen_classes, w2v_path, test_data_path)\n",
    "embedding = train_set.embedding\n",
    "categorical = train_set.categorical\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=4, collate_fn=batch_function, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=4, collate_fn=batch_function, num_workers=4)\n",
    "\n",
    "# load settings\n",
    "config = setting(train_set, test_set, embedding)\n",
    "\n",
    "# Training cycle\n",
    "overall_train_time = 0.0\n",
    "overall_test_time = 0.0\n",
    "\n",
    "model = CapsuleNetwork(config).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "if os.path.exists(config['ckpt_dir'] + 'best_model.pth'):\n",
    "    print(\"Restoring weights from previously trained rnn model.\")\n",
    "    model.load_state_dict(torch.load(config['ckpt_dir'] + 'best_model.pth' ))\n",
    "else:\n",
    "    print('Initializing Variables')\n",
    "    if not os.path.exists(config['ckpt_dir']):\n",
    "        os.mkdir(config['ckpt_dir'])\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1200/2472 | Loss: 13.039449691772461 | Acc: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f13f75d20d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dl/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/dl/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, config['num_epochs'] + 1):\n",
    "    model.train()\n",
    "    avg_acc = 0\n",
    "    epoch_time = time.time()\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # sort by descending order for pack_padded_sequence\n",
    "        input = batch.sentences_w2v\n",
    "        lengths = batch.lengths\n",
    "        target = batch.label_onehot\n",
    "        label_w2v = batch.label_w2v\n",
    "\n",
    "        output = model.forward(input, lengths, embedding)\n",
    "        loss_val = model.loss(target.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        clone_logits = model.logits.detach().clone()\n",
    "        pred = np.argmax(clone_logits, 1)\n",
    "        acc = accuracy_score(categorical(target), pred)\n",
    "        clear_output(wait=True)\n",
    "        print(\"Batch: {}/{} | Loss: {} | Acc: {}\".format((idx+1), len(train_loader), loss_val.item(), acc))\n",
    "        avg_acc += acc\n",
    "\n",
    "    train_time = time.time() - epoch_time\n",
    "    overall_train_time += train_time\n",
    "    print(\"------epoch : \", epoch, \" Loss: \", loss_val.item(), \" Acc:\", round((avg_acc / batch_num), 4), \" Train time: \", round(train_time, 4), \"--------\")\n",
    "\n",
    "    model.eval()\n",
    "    cur_acc = evaluate_test(data, config, model,torch.from_numpy(embedding))\n",
    "    if cur_acc > best_acc:\n",
    "        # save model\n",
    "        best_acc = cur_acc\n",
    "        torch.save(model.state_dict(), config['ckpt_dir'] + 'best_model.pth')\n",
    "\n",
    "    print(\"cur_acc\", cur_acc)\n",
    "    print(\"best_acc\", best_acc)\n",
    "    test_time = time.time() - epoch_time\n",
    "    overall_test_time += test_time\n",
    "    print(\"Testing time\", round(test_time, 4))\n",
    "\n",
    "print(\"Overall training time\", overall_train_time)\n",
    "print(\"Overall testing time\", overall_test_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
